# -*- coding: utf-8 -*-
"""channel_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13LBTtW36uw33Jd5Lazem3Fgl2sztiQxd
"""

import time
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import tensorflow as tf
from tensorflow.python.compiler.tensorrt import trt_convert as trt

class ResidualBlock(tf.keras.Model):
    def __init__(self, filters, data_format):
        super().__init__()
        axis = -1 if data_format == 'channels_last' else 1
        self.bn1 = tf.keras.layers.BatchNormalization(axis=axis)
        self.act1 = tf.keras.layers.ReLU()
        self.conv1 = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same', data_format=data_format, use_bias=False)
        self.bn2 = tf.keras.layers.BatchNormalization(axis=axis)
        self.act2 = tf.keras.layers.ReLU()
        self.conv2 = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same', data_format=data_format, use_bias=True)

    def call(self, x):
        h = self.bn1(x)
        h = self.act1(h)
        h = self.conv1(h)
        h = self.bn2(h)
        h = self.act2(h)
        h = self.conv2(h)
        y = x + h
        return y

class Entry(tf.keras.Model):
    def __init__(self, filters, data_format):
        super().__init__()
        self.conv1a = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same', data_format=data_format, use_bias=False)
        self.conv1b = tf.keras.layers.Conv2D(filters=filters, kernel_size=1, padding='same', data_format=data_format, use_bias=False)
        self.conv2 = tf.keras.layers.Conv2D(filters=filters, kernel_size=1, padding='same', data_format=data_format, use_bias=False)
        axis = -1 if data_format == 'channels_last' else 1
        self.bn = tf.keras.layers.BatchNormalization(axis=axis)
        self.act = tf.keras.layers.ReLU()
        self.conv3 = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same', data_format=data_format, use_bias=True)

    def call(self, x):
        x1, x2 = x
        h = self.conv1a(x1) + self.conv1b(x1) + self.conv2(x2)
        h = self.bn(h)
        h = self.act(h)
        y = self.conv3(h)
        return y

class Bias(tf.keras.layers.Layer):
    def __init__(self, data_format):
        super().__init__()

    def build(self, input_shape):
        self.bias = self.add_weight(shape=(27 * 9 * 9,), name='bias')

    def call(self, x):
        return x + self.bias


class Policy(tf.keras.Model):
    def __init__(self, data_format):
        super().__init__()
        self.conv = tf.keras.layers.Conv2D(filters=27, kernel_size=1, padding='same', data_format=data_format, use_bias=False)
        self.flatten = tf.keras.layers.Flatten(data_format=data_format)
        self.bias = Bias(data_format)

    def call(self, x):
        h = self.conv(x)
        h = self.flatten(h)
        y = self.bias(h)
        return y


class Value(tf.keras.Model):
    def __init__(self, filters, data_format):
        super().__init__()
        axis = -1 if data_format == 'channels_last' else 1
        self.conv = tf.keras.layers.Conv2D(filters=filters, kernel_size=1, padding='same', data_format=data_format, use_bias=False)
        self.bn = tf.keras.layers.BatchNormalization(axis=axis)
        self.act = tf.keras.layers.ReLU()
        self.flatten = tf.keras.layers.Flatten(data_format=data_format)
        self.dense1 = tf.keras.layers.Dense(units=256, activation='relu')
        self.dense2 = tf.keras.layers.Dense(units=1)

    def call(self, x):
        h = self.conv(x)
        h = self.bn(h)
        h = self.act(h)
        h = self.flatten(h)
        h = self.dense1(h)
        y = self.dense2(h)
        return y

class PolicyValueNetwork(tf.keras.Model):
    def __init__(self, filters, blocks, data_format):
        super().__init__()
        layers = [Entry(filters=filters, data_format=data_format)]
        for _ in range(blocks):
            layers.append(ResidualBlock(filters=filters, data_format=data_format))
        self.stem = tf.keras.Sequential(layers)
        self.policy = Policy(data_format=data_format)
        self.value = Value(filters=filters, data_format=data_format)

    def call(self, x):
        h = self.stem(x)
        policy = self.policy(h)
        value = self.value(h)
        return policy, value


def generate_data(batch_size, data_format):
    if data_format == 'channels_last':
        data1 = np.random.rand(batch_size, 9, 9, 62).astype(np.float32)
        data2 = np.random.rand(batch_size, 9, 9, 57).astype(np.float32)
    else:
        data1 = np.random.rand(batch_size, 62, 9, 9).astype(np.float32)
        data2 = np.random.rand(batch_size, 57, 9, 9).astype(np.float32)
    return data1, data2


def run(batch_size, data_format):
    data1, data2 = generate_data(batch_size=batch_size, data_format=data_format)

    model = PolicyValueNetwork(256, 20, data_format)
    _ = model((data1, data2))

    output_dir = f'tf_{data_format}.keras'
    # model.save(output_dir)
    tf.saved_model.save(model, output_dir)
    # model.summary()

    # Conversion Parameters
    conversion_params = trt.TrtConversionParams(precision_mode=trt.TrtPrecisionMode.FP32,
    						use_calibration=True,
    						maximum_cached_engines=100)
    converter = trt.TrtGraphConverterV2(input_saved_model_dir=output_dir,
                                        conversion_params=conversion_params,
                                        use_dynamic_shape=True)

    def my_input_fn():
        if data_format == 'channels_last':
            data1 = np.zeros((256, 9, 9, 62)).astype(np.float32)
            data2 = np.zeros((256, 9, 9, 57)).astype(np.float32)
        else:
            data1 = np.zeros((256, 62, 9, 9)).astype(np.float32)
            data2 = np.zeros((256, 57, 9, 9)).astype(np.float32)
        yield [data1,  data2]

    trt_func = converter.convert()
    converter.summary()

    # Optionally, build TensorRT engines before deployment to save time at runtime
    # Note that this is GPU specific, and as a rule of thumb, we recommend building at runtime
    # converter.build(input_fn=my_input_fn)

    # Save the model to the disk
    converter.save(f'rt_{data_format}')

    saved_model = tf.saved_model.load(f'rt_{data_format}')
    infer = saved_model.signatures['serving_default']
    _ = infer(input_1=data1, input_2=data2)


def check(batch_size, data_format, n=100):
    data1, data2 = generate_data(batch_size=batch_size, data_format=data_format)

    model = PolicyValueNetwork(256, 20, data_format)
    _ = model((data1, data2))

    start = time.time()
    for _ in range(n):
        data = generate_data(batch_size=batch_size, data_format=data_format)
        _ = model(data)
    duration = time.time() - start
    print(f'tensorflow model {data_format}: {duration} s')
    del model

    saved_model = tf.saved_model.load(f'rt_{data_format}')
    infer = saved_model.signatures['serving_default']
    _ = infer(input_1=data1, input_2=data2)

    start = time.time()
    for _ in range(n):
        data = generate_data(batch_size=batch_size, data_format=data_format)
        _ = infer(input_1=data1, input_2=data2)
    duration = time.time() - start
    print(f'tensorrt model {data_format}: {duration} s')


if __name__ == '__main__':
    for data_format in ('channels_last', 'channels_first'):
        run(20, data_format)
        check(128, data_format, n=500)       
